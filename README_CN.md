# åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆå…ƒå­¦ä¹ å¢å¼ºç‰ˆï¼‰

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶åŠ¨åŠ›å­¦æ¨¡å‹è¯¯å·®åœ¨çº¿è¡¥å¿æ–¹æ³•çš„å®˜æ–¹å®ç°ã€‚

[English](README.md) | ç®€ä½“ä¸­æ–‡

## ğŸ“– è®ºæ–‡å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```bibtex
@article{wu2025rl,
  title={Reinforcement Learning-Enhanced Model Predictive Control with Meta-Learning for Online Compensation of Dynamic Model Errors},
  author={Wu, Jiahao and others},
  journal={To be published},
  year={2025},
  note={Manuscript in preparation}
}
```

## ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹

- **å…ƒå­¦ä¹ ç½‘ç»œ**ï¼šå¿«é€Ÿè‡ªé€‚åº”PIDå‚æ•°é¢„æµ‹ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒå‚
- **å¼ºåŒ–å­¦ä¹ å¢å¼º**ï¼šåœ¨çº¿å®æ—¶è¡¥å¿åŠ¨åŠ›å­¦æ¨¡å‹è¯¯å·®
- **å¤šæœºå™¨äººå¹³å°**ï¼šæ”¯æŒFranka Pandaï¼ˆ9è‡ªç”±åº¦ä¸²è”æœºæ¢°è‡‚ï¼‰å’ŒLaikagoï¼ˆ12è‡ªç”±åº¦å¹¶è”å››è¶³æœºå™¨äººï¼‰
- **é²æ£’æ€§éªŒè¯**ï¼šæŠ—å¤–éƒ¨æ‰°åŠ¨å’Œæ¨¡å‹ä¸ç¡®å®šæ€§
- **æ•°æ®å¢å¼ºæŠ€æœ¯**ï¼šåŸºäºç‰©ç†çº¦æŸçš„è™šæ‹Ÿæ ·æœ¬ç”Ÿæˆï¼Œæå‡æ³›åŒ–èƒ½åŠ›

## ğŸ¯ ä¸»è¦æˆæœ

### Franka Pandaå¹³å°ï¼ˆ9è‡ªç”±åº¦ï¼‰
| æŒ‡æ ‡ | çº¯Meta-PID | Meta-PID+RL | æ”¹è¿›ç‡ |
|------|------------|-------------|---------|
| MAE (Â°) | 7.51 | **6.26** | **+16.6%** |
| RMSE (Â°) | 29.32 | **25.45** | **+13.2%** |
| æœ€å¤§è¯¯å·® (Â°) | 48.49 | **42.12** | **+13.1%** |

### Laikagoå¹³å°ï¼ˆ12è‡ªç”±åº¦ï¼‰
| æŒ‡æ ‡ | çº¯Meta-PID | Meta-PID+RL | æ”¹è¿›ç‡ |
|------|------------|-------------|---------|
| MAE (Â°) | 5.91 | **5.91** | **0.0%** |
| RMSE (Â°) | 13.80 | **13.74** | **+0.4%** |

**é‡è¦å‘ç°**ï¼šLaikagoå¹³å°çš„0.0% MAEæ”¹è¿›ç‡åæ˜ äº†"ä¼˜åŒ–å¤©èŠ±æ¿æ•ˆåº”"â€”â€”å½“å…ƒå­¦ä¹ åŸºçº¿å·²ç»åœ¨æ‰€æœ‰å…³èŠ‚ä¸Šè¾¾åˆ°å‡è¡¡ä¸”æ¥è¿‘æœ€ä¼˜çš„æ€§èƒ½æ—¶ï¼Œå¼ºåŒ–å­¦ä¹ çš„è¾¹é™…æ”¶ç›Šå—åˆ°é™åˆ¶ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£å…ƒå­¦ä¹ ä¸RLååŒçš„è¾¹ç•Œæ¡ä»¶æä¾›äº†é‡è¦æ´å¯Ÿã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- PyBulletï¼ˆç‰©ç†ä»¿çœŸå¼•æ“ï¼‰
- NumPy, Matplotlibï¼ˆæ•°æ®å¤„ç†ä¸å¯è§†åŒ–ï¼‰

### å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/WUJIAHAO-HKU/rl-mpc-meta-learning.git
cd rl-mpc-meta-learning

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
conda create -n rl-mpc python=3.9
conda activate rl-mpc

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–è€…ä»¥å¼€å‘æ¨¡å¼å®‰è£…
pip install -e .
```

### åŸºæœ¬ä½¿ç”¨

#### 1ï¸âƒ£ è®­ç»ƒå…ƒå­¦ä¹ ç½‘ç»œ

```bash
# ä½¿ç”¨æ•°æ®å¢å¼ºè®­ç»ƒï¼ˆæ¨èï¼‰
python src/training/train_with_augmentation.py

# ä½¿ç”¨åŸå§‹æ•°æ®è®­ç»ƒ
python src/training/train_meta_pid.py
```

#### 2ï¸âƒ£ è®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥

```bash
# Franka Pandaæœºæ¢°è‡‚
python src/training/train_meta_rl_combined.py \
    --robot franka \
    --timesteps 1000000 \
    --meta_model models/meta_pid_augmented.pth

# Laikagoå››è¶³æœºå™¨äºº
python src/training/train_meta_rl_combined.py \
    --robot laikago \
    --timesteps 1000000 \
    --meta_model models/meta_pid_augmented.pth
```

#### 3ï¸âƒ£ è¯„ä¼°æ€§èƒ½

```bash
# è¯„ä¼°Franka Panda
python src/evaluation/evaluate_meta_rl.py \
    --robot franka \
    --model models/franka_rl_policy.zip \
    --n_episodes 100

# è¯„ä¼°Laikago
python src/evaluation/evaluate_laikago.py \
    --model models/laikago_rl_policy.zip \
    --n_episodes 100

# é²æ£’æ€§æµ‹è¯•ï¼ˆæŠ—æ‰°åŠ¨èƒ½åŠ›ï¼‰
python src/evaluation/evaluate_robustness.py \
    --robot franka \
    --disturbance_level 0.3
```

#### 4ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–ç»“æœ

```bash
# ç”Ÿæˆæ‰€æœ‰è®ºæ–‡å›¾è¡¨
python src/visualization/generate_all_figures_unified.py

# å¯è§†åŒ–è®­ç»ƒæ›²çº¿
python src/visualization/visualize_training_curves.py \
    --log logs/training.log
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
rl-mpc-meta-learning/
â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â”œâ”€â”€ networks/                 # ç¥ç»ç½‘ç»œæ¶æ„å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ meta_pid_network.py  # å…ƒå­¦ä¹ PIDé¢„æµ‹å™¨
â”‚   â”‚   â””â”€â”€ rl_policy.py         # RLç­–ç•¥ç½‘ç»œ
â”‚   â”œâ”€â”€ environments/             # PyBulletä»¿çœŸç¯å¢ƒ
â”‚   â”‚   â”œâ”€â”€ base_env.py          # åŸºç¡€ç¯å¢ƒç±»
â”‚   â”‚   â”œâ”€â”€ meta_rl_combined_env.py  # å…ƒå­¦ä¹ +RLç¯å¢ƒ
â”‚   â”‚   â””â”€â”€ meta_rl_disturbance_env.py  # æ‰°åŠ¨æµ‹è¯•ç¯å¢ƒ
â”‚   â”œâ”€â”€ training/                 # è®­ç»ƒè„šæœ¬
â”‚   â”‚   â”œâ”€â”€ train_meta_pid.py    # å…ƒå­¦ä¹ ç½‘ç»œè®­ç»ƒ
â”‚   â”‚   â”œâ”€â”€ train_with_augmentation.py  # æ•°æ®å¢å¼ºè®­ç»ƒ
â”‚   â”‚   â””â”€â”€ train_meta_rl_combined.py   # RLè®­ç»ƒ
â”‚   â”œâ”€â”€ evaluation/               # è¯„ä¼°è„šæœ¬
â”‚   â”‚   â”œâ”€â”€ evaluate_meta_rl.py  # æ€§èƒ½è¯„ä¼°
â”‚   â”‚   â””â”€â”€ evaluate_robustness.py  # é²æ£’æ€§æµ‹è¯•
â”‚   â””â”€â”€ visualization/            # å¯è§†åŒ–å·¥å…·
â”‚       â”œâ”€â”€ generate_all_figures_unified.py  # ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
â”‚       â””â”€â”€ visualize_training_curves.py     # è®­ç»ƒæ›²çº¿å¯è§†åŒ–
â”œâ”€â”€ data/                         # æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ augmented_pid_data.json  # å¢å¼ºåçš„è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ best_configs_paper.json  # è®ºæ–‡ä¸­çš„æœ€ä½³é…ç½®
â”œâ”€â”€ configs/                      # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ training_config.yaml     # è®­ç»ƒé…ç½®
â”œâ”€â”€ models/                       # é¢„è®­ç»ƒæ¨¡å‹ï¼ˆé€šè¿‡Releaseä¸‹è½½ï¼‰
â”œâ”€â”€ results/                      # å®éªŒç»“æœ
â”œâ”€â”€ scripts/                      # å®ç”¨è„šæœ¬
â”‚   â””â”€â”€ reproduce_paper_results.sh  # å®Œæ•´å¤ç°è„šæœ¬
â”œâ”€â”€ tests/                        # å•å…ƒæµ‹è¯•
â”œâ”€â”€ README.md                     # è‹±æ–‡README
â”œâ”€â”€ README_CN.md                  # ä¸­æ–‡READMEï¼ˆæœ¬æ–‡ä»¶ï¼‰
â”œâ”€â”€ QUICK_START.md                # å¿«é€Ÿå…¥é—¨æŒ‡å—
â”œâ”€â”€ GITHUB_UPLOAD_GUIDE.md        # GitHubä¸Šä¼ æ•™ç¨‹
â”œâ”€â”€ requirements.txt              # Pythonä¾èµ–
â”œâ”€â”€ setup.py                      # å®‰è£…é…ç½®
â””â”€â”€ LICENSE                       # MITè®¸å¯è¯
```

## ğŸ”¬ ç®—æ³•åŸç†

### ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ§åˆ¶ç³»ç»Ÿæ€»è§ˆ                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  æœºå™¨äººçŠ¶æ€ s_t â”€â”€â†’ [å…ƒå­¦ä¹ ç½‘ç»œ] â”€â”€â†’ PIDå¢ç›Š [K_p,K_i,K_d]  â”‚
â”‚                           â†“                                   â”‚
â”‚                    [PIDæ§åˆ¶å™¨] â”€â”€â†’ Ï„_base                    â”‚
â”‚                           â†“                                   â”‚
â”‚  å¢å¼ºçŠ¶æ€ â”€â”€â”€â”€â”€â”€â”€â”€â†’ [RLç­–ç•¥] â”€â”€â†’ Î´Ï„ (è¡¥å¿åŠ›çŸ©)              â”‚
â”‚                           â†“                                   â”‚
â”‚                    Ï„_total = Ï„_base + Î´Ï„                     â”‚
â”‚                           â†“                                   â”‚
â”‚                      [æœºå™¨äººæ‰§è¡Œ]                             â”‚
â”‚                           â†“                                   â”‚
â”‚                    è·Ÿè¸ªè¯¯å·®åé¦ˆ                                â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. å…ƒå­¦ä¹ PIDé¢„æµ‹å™¨

æ ¹æ®æœºå™¨äººå½“å‰çŠ¶æ€è‡ªé€‚åº”é¢„æµ‹PIDå¢ç›Šï¼š

**è¾“å…¥**: `s_t = [q, qÌ‡, q_ref, qÌ‡_ref]` (å…³èŠ‚ä½ç½®ã€é€Ÿåº¦åŠå‚è€ƒå€¼)  
**è¾“å‡º**: `[K_p, K_i, K_d]` (æ¯ä¸ªå…³èŠ‚çš„PIDå¢ç›Š)

```python
class MetaPIDNetwork(nn.Module):
    def __init__(self, state_dim=4, hidden_dim=64, output_dim=3):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softplus()  # ç¡®ä¿PIDå¢ç›Šä¸ºæ­£
        )
```

### 2. RLç­–ç•¥ç½‘ç»œ

åœ¨å…ƒå­¦ä¹ PIDåŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¡¥å¿æ¨¡å‹è¯¯å·®ï¼š

**è¾“å…¥**: å¢å¼ºçŠ¶æ€ `[s_t, K_p, K_i, K_d, tracking_error]`  
**è¾“å‡º**: è¡¥å¿åŠ›çŸ© `Î´Ï„`

```python
class RLPolicy(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Tanh()  # é™åˆ¶åŠ¨ä½œèŒƒå›´ [-1, 1]
        )
```

### 3. å¥–åŠ±å‡½æ•°è®¾è®¡

å¤šç›®æ ‡ä¼˜åŒ–å¥–åŠ±å‡½æ•°ï¼š

```python
def compute_reward(tracking_error, action, prev_error):
    """
    r_total = r_tracking + r_smoothness + r_improvement
    """
    # 1. è·Ÿè¸ªç²¾åº¦ï¼ˆä¸»è¦ç›®æ ‡ï¼‰
    r_tracking = -np.linalg.norm(tracking_error)
    
    # 2. åŠ¨ä½œå¹³æ»‘æ€§ï¼ˆé¿å…æŠ–åŠ¨ï¼‰
    r_smoothness = -0.1 * np.linalg.norm(action)
    
    # 3. è¯¯å·®å‡å°‘å¥–åŠ±ï¼ˆé¼“åŠ±æ”¹è¿›ï¼‰
    r_improvement = 10.0 * (np.linalg.norm(prev_error) - 
                           np.linalg.norm(tracking_error))
    
    return r_tracking + r_smoothness + r_improvement
```

### 4. æ•°æ®å¢å¼ºç­–ç•¥

åŸºäºç‰©ç†çº¦æŸç”Ÿæˆè™šæ‹Ÿæ ·æœ¬ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ï¼š

- **çŠ¶æ€å™ªå£°æ³¨å…¥**ï¼š`s' = s + Îµ`, `Îµ ~ N(0, ÏƒÂ²)`
- **PIDå¢ç›Šæ‰°åŠ¨**ï¼šä¿æŒç¨³å®šæ€§çº¦æŸ
- **å¯æ§æ€§æ£€æŸ¥**ï¼šç¡®ä¿ç”Ÿæˆçš„æ ·æœ¬ç‰©ç†å¯è¡Œ

## ğŸ“Š å®Œæ•´å®éªŒå¤ç°

### ä¸€é”®å¤ç°ï¼ˆæ¨èï¼‰

```bash
# è¿è¡Œå®Œæ•´å®éªŒæµç¨‹ï¼ˆçº¦24å°æ—¶ï¼Œå•GPUï¼‰
bash scripts/reproduce_paper_results.sh
```

### åˆ†æ­¥æ‰§è¡Œ

```bash
# Step 1: è®­ç»ƒå…ƒå­¦ä¹ ç½‘ç»œ
python src/training/train_with_augmentation.py

# Step 2: è®­ç»ƒFranka RLç­–ç•¥
python src/training/train_meta_rl_combined.py --robot franka --timesteps 1000000

# Step 3: è®­ç»ƒLaikago RLç­–ç•¥
python src/training/train_meta_rl_combined.py --robot laikago --timesteps 1000000

# Step 4: è¯„ä¼°æ€§èƒ½
python src/evaluation/evaluate_meta_rl.py --robot franka
python src/evaluation/evaluate_laikago.py

# Step 5: ç”Ÿæˆå›¾è¡¨
python src/visualization/generate_all_figures_unified.py
```

## ğŸ“ æ•™ç¨‹ä¸ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šè‡ªå®šä¹‰æœºå™¨äººå¹³å°

```python
from src.environments.base_env import BaseRobotEnv

class MyRobotEnv(BaseRobotEnv):
    """è‡ªå®šä¹‰æ‚¨çš„æœºå™¨äººç¯å¢ƒ"""
    def __init__(self):
        super().__init__(
            urdf_path="path/to/your/robot.urdf",
            n_joints=7,
            max_torque=100.0,
            control_freq=240
        )
    
    def compute_dynamics(self, q, q_dot):
        """å®ç°æ‚¨çš„åŠ¨åŠ›å­¦æ¨¡å‹"""
        # è®¡ç®—è´¨é‡çŸ©é˜µã€ç§‘é‡Œå¥¥åˆ©åŠ›ç­‰
        M = self.compute_mass_matrix(q)
        C = self.compute_coriolis(q, q_dot)
        G = self.compute_gravity(q)
        return M, C, G
```

### ç¤ºä¾‹2ï¼šè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

```python
def custom_reward_function(state, action, next_state):
    """æ ¹æ®æ‚¨çš„ä»»åŠ¡å®šåˆ¶å¥–åŠ±"""
    # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®è¯¯å·®
    ee_error = np.linalg.norm(state['ee_pos'] - state['target_pos'])
    r_position = -ee_error
    
    # èƒ½é‡æ¶ˆè€—æƒ©ç½š
    r_energy = -0.01 * np.sum(action**2)
    
    # ä»»åŠ¡å®Œæˆå¥–åŠ±
    r_success = 100.0 if ee_error < 0.01 else 0.0
    
    return r_position + r_energy + r_success
```

### ç¤ºä¾‹3ï¼šå¯è§†åŒ–æœºå™¨äººè¿åŠ¨

```python
from src.environments.meta_rl_combined_env import MetaRLCombinedEnv
import pybullet as p

# åˆ›å»ºç¯å¢ƒï¼ˆå¼€å¯GUIï¼‰
env = MetaRLCombinedEnv(robot='franka', render=True)
obs, info = env.reset()

# è¿è¡Œæ§åˆ¶å¾ªç¯
for step in range(1000):
    # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = env.step(action)
    
    if done:
        print(f"Episode finished! Tracking error: {info['tracking_error']:.3f}Â°")
        break

env.close()
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### è¶…å‚æ•°è°ƒä¼˜

ç¼–è¾‘ `configs/training_config.yaml`:

```yaml
meta_learning:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  
reinforcement_learning:
  algorithm: PPO
  total_timesteps: 1000000
  learning_rate: 3e-4
```

### ä½¿ç”¨TensorBoardç›‘æ§è®­ç»ƒ

```bash
# å¯åŠ¨è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨è®°å½•åˆ°logs/
python src/training/train_meta_rl_combined.py --robot franka

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯å¯åŠ¨TensorBoard
tensorboard --logdir=logs/

# æµè§ˆå™¨è®¿é—®: http://localhost:6006
```

### åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šGPUï¼‰

```python
# åœ¨train_meta_rl_combined.pyä¸­è®¾ç½®
from stable_baselines3.common.vec_env import SubprocVecEnv

# åˆ›å»ºå¤šä¸ªå¹¶è¡Œç¯å¢ƒ
n_envs = 4
envs = [make_env(i) for i in range(n_envs)]
vec_env = SubprocVecEnv(envs)

# è®­ç»ƒ
model = PPO("MlpPolicy", vec_env, device="cuda", n_steps=2048)
model.learn(total_timesteps=1000000)
```

## ğŸ§ª æµ‹è¯•

è¿è¡Œå•å…ƒæµ‹è¯•ï¼š

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
python -m pytest tests/test_meta_pid_network.py -v

# æ£€æŸ¥ä»£ç è¦†ç›–ç‡
pytest --cov=src tests/
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨æ•°æ®å¢å¼º**ï¼šå¯æå‡çº¦5-10%çš„æ³›åŒ–æ€§èƒ½
2. **è°ƒæ•´å­¦ä¹ ç‡**ï¼šå¯¹äºä¸åŒæœºå™¨äººï¼Œæœ€ä¼˜å­¦ä¹ ç‡å¯èƒ½ä¸åŒ
3. **å¢åŠ è®­ç»ƒæ­¥æ•°**ï¼šå¤æ‚ä»»åŠ¡å¯èƒ½éœ€è¦2M+æ­¥
4. **è¯¾ç¨‹å­¦ä¹ **ï¼šä»ç®€å•è½¨è¿¹é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚è½¨è¿¹
5. **é›†æˆå­¦ä¹ **ï¼šè®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶é›†æˆå¯æé«˜é²æ£’æ€§

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯Pull Request

è¯·ç¡®ä¿ï¼š
- ä»£ç ç¬¦åˆPEP 8è§„èŒƒ
- æ·»åŠ é€‚å½“çš„æ³¨é‡Šå’Œæ–‡æ¡£
- é€šè¿‡æ‰€æœ‰å•å…ƒæµ‹è¯•
- æ›´æ–°ç›¸å…³çš„README

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ“§ è”ç³»æ–¹å¼

- **ä½œè€…**ï¼šWU JIAHAO
- **é‚®ç®±**ï¼šu3661739@connect.hku.hk
- **é¡¹ç›®ä¸»é¡µ**ï¼š[https://github.com/WUJIAHAO-HKU/rl-mpc-meta-learning](https://github.com/WUJIAHAO-HKU/rl-mpc-meta-learning)
- **é—®é¢˜åé¦ˆ**ï¼š[GitHub Issues](https://github.com/WUJIAHAO-HKU/rl-mpc-meta-learning/issues)

## ğŸ™ è‡´è°¢

- PyBulletå›¢é˜Ÿæä¾›çš„ä¼˜ç§€ç‰©ç†ä»¿çœŸå¼•æ“
- Stable-Baselines3æä¾›çš„é«˜è´¨é‡RLç®—æ³•å®ç°
- åŒ¿åå®¡ç¨¿äººçš„å®è´µæ„è§å’Œå»ºè®¾æ€§å»ºè®®
- å¼€æºç¤¾åŒºçš„æ”¯æŒä¸è´¡çŒ®

## ğŸ“š ç›¸å…³èµ„æº

### ç›¸å…³è®ºæ–‡
- [Meta-Learning for Control](https://arxiv.org/abs/xxxx.xxxxx)
- [Model Predictive Control with Deep Learning](https://arxiv.org/abs/xxxx.xxxxx)
- [Reinforcement Learning for Robotics: A Survey](https://arxiv.org/abs/xxxx.xxxxx)

### æ¨èé˜…è¯»
- [PyBulletå®˜æ–¹æ–‡æ¡£](https://pybullet.org/)
- [Stable-Baselines3æ–‡æ¡£](https://stable-baselines3.readthedocs.io/)
- [PyTorchå®˜æ–¹æ•™ç¨‹](https://pytorch.org/tutorials/)

## ğŸ”„ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-11-02)
- ğŸ‰ åˆå§‹å‘å¸ƒ
- âœ… æ”¯æŒFranka Pandaå’ŒLaikagoä¸¤ä¸ªå¹³å°
- âœ… å®Œæ•´çš„å…ƒå­¦ä¹ +RLè®­ç»ƒæµç¨‹
- âœ… æ•°æ®å¢å¼ºåŠŸèƒ½
- âœ… é²æ£’æ€§æµ‹è¯•
- âœ… è¯¦ç»†çš„æ–‡æ¡£å’Œæ•™ç¨‹

## â“ å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰

<details>
<summary><b>Q: è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ</b></summary>

A: 
- å…ƒå­¦ä¹ ç½‘ç»œè®­ç»ƒï¼šçº¦1-2å°æ—¶ï¼ˆCPUï¼‰æˆ–10-20åˆ†é’Ÿï¼ˆGPUï¼‰
- RLç­–ç•¥è®­ç»ƒï¼ˆ1Mæ­¥ï¼‰ï¼šçº¦8-12å°æ—¶ï¼ˆå•GPUï¼‰
- å®Œæ•´æµç¨‹ï¼šçº¦24å°æ—¶
</details>

<details>
<summary><b>Q: éœ€è¦ä»€ä¹ˆç¡¬ä»¶é…ç½®ï¼Ÿ</b></summary>

A:
- æœ€ä½é…ç½®ï¼š16GB RAM, 4æ ¸CPU
- æ¨èé…ç½®ï¼š32GB RAM, 8æ ¸CPU, NVIDIA GPU (8GB+ VRAM)
- GPUä¸æ˜¯å¿…é¡»çš„ï¼Œä½†èƒ½æ˜¾è‘—åŠ é€Ÿè®­ç»ƒ
</details>

<details>
<summary><b>Q: å¦‚ä½•é€‚é…æ–°çš„æœºå™¨äººï¼Ÿ</b></summary>

A: 
1. å‡†å¤‡URDFæ–‡ä»¶
2. ç»§æ‰¿`BaseRobotEnv`ç±»
3. å®ç°åŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºæ¨¡æ‹Ÿï¼‰
4. å‚è€ƒç¤ºä¾‹ä»£ç è¿›è¡Œè®­ç»ƒ
</details>

<details>
<summary><b>Q: ä¸ºä»€ä¹ˆLaikagoçš„æ”¹è¿›ç‡æ˜¯0.0%ï¼Ÿ</b></summary>

A: 
è¿™æ˜¯"ä¼˜åŒ–å¤©èŠ±æ¿æ•ˆåº”"çš„ä½“ç°ã€‚å½“å…ƒå­¦ä¹ åŸºçº¿å·²ç»éå¸¸ä¼˜ç§€ï¼ˆå„å…³èŠ‚å‡è¡¡ã€æ¥è¿‘æœ€ä¼˜ï¼‰æ—¶ï¼ŒRLçš„æ”¹è¿›ç©ºé—´æœ‰é™ã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ç§‘å­¦å‘ç°ï¼Œè¯´æ˜äº†å…ƒå­¦ä¹ ä¸RLååŒçš„è¾¹ç•Œæ¡ä»¶ã€‚
</details>

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªStarï¼â­**

**ğŸŒ æ¬¢è¿å…³æ³¨æˆ‘ä»¬çš„å·¥ä½œï¼Œå…±åŒæ¨è¿›æœºå™¨äººæ§åˆ¶æŠ€æœ¯çš„å‘å±•ï¼**

