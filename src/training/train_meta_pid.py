"""
å…ƒå­¦ä¹ PIDæ¨¡å‹è®­ç»ƒè„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½è®­ç»ƒæ•°æ®é›†
2. è®­ç»ƒå…ƒå­¦ä¹ ç½‘ç»œ
3. è¯„ä¼°é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from meta_learning.meta_pid_optimizer import MetaPIDOptimizer, MetaPIDNetwork, RobotFeatureExtractor


class PIDDataset(Dataset):
    """PIDå‚æ•°æ•°æ®é›†"""
    
    def __init__(self, data_points, feature_extractor, normalization_stats=None):
        """
        Args:
            data_points: list of dict, æ¯ä¸ªåŒ…å«featureså’Œoptimal_pid
            feature_extractor: RobotFeatureExtractorå®ä¾‹
            normalization_stats: å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆå¯é€‰ï¼‰
        """
        self.data_points = data_points
        self.feature_extractor = feature_extractor
        
        # æå–æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
        self.features = []
        self.labels = []
        self.dofs = []
        
        for dp in data_points:
            # ç‰¹å¾
            feature_dict = dp['features']
            feature_vec = np.array([feature_dict[name] 
                                   for name in feature_extractor.feature_names], 
                                  dtype=np.float32)
            
            # æ ‡ç­¾ï¼ˆPIDå‚æ•°ï¼‰
            pid = dp['optimal_pid']
            kp = np.array(pid['Kp'], dtype=np.float32)
            ki = np.array(pid['Ki'], dtype=np.float32)
            kd = np.array(pid['Kd'], dtype=np.float32)
            
            # Padåˆ°æœ€å¤§DOFï¼ˆ7ï¼‰
            max_dof = 7
            actual_dof = len(kp)
            
            kp_padded = np.zeros(max_dof, dtype=np.float32)
            ki_padded = np.zeros(max_dof, dtype=np.float32)
            kd_padded = np.zeros(max_dof, dtype=np.float32)
            
            kp_padded[:actual_dof] = kp
            ki_padded[:actual_dof] = ki
            kd_padded[:actual_dof] = kd
            
            self.features.append(feature_vec)
            self.labels.append(np.stack([kp_padded, ki_padded, kd_padded]))  # (3, max_dof)
            self.dofs.append(actual_dof)
        
        self.features = np.array(self.features)
        self.labels = np.array(self.labels)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        if normalization_stats is None:
            self.mean = np.mean(self.features, axis=0)
            self.std = np.std(self.features, axis=0) + 1e-8
        else:
            self.mean = normalization_stats['mean']
            self.std = normalization_stats['std']
        
        self.features = (self.features - self.mean) / self.std
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return (
            torch.FloatTensor(self.features[idx]),
            torch.FloatTensor(self.labels[idx]),
            self.dofs[idx]
        )
    
    def get_normalization_stats(self):
        return {'mean': self.mean, 'std': self.std}


def weighted_mse_loss(pred, target, dof, max_dof=7):
    """
    åŠ æƒMSEæŸå¤±ï¼ˆåªè®¡ç®—å®é™…DOFçš„æŸå¤±ï¼‰
    
    Args:
        pred: (batch, 3, max_dof) é¢„æµ‹çš„PIDå‚æ•°
        target: (batch, 3, max_dof) ç›®æ ‡PIDå‚æ•°
        dof: (batch,) å®é™…è‡ªç”±åº¦
        max_dof: æœ€å¤§è‡ªç”±åº¦
    
    Returns:
        loss: æ ‡é‡æŸå¤±
    """
    batch_size = pred.shape[0]
    
    # åˆ›å»ºmask
    mask = torch.zeros_like(pred)
    for i, d in enumerate(dof):
        mask[i, :, :d] = 1.0
    
    # åŠ æƒMSE
    squared_diff = (pred - target) ** 2
    masked_loss = squared_diff * mask
    
    # å½’ä¸€åŒ–ï¼ˆé™¤ä»¥å®é™…å…ƒç´ æ•°é‡ï¼‰
    num_elements = mask.sum()
    loss = masked_loss.sum() / (num_elements + 1e-8)
    
    return loss


def relative_error_loss(pred, target, dof):
    """
    ç›¸å¯¹è¯¯å·®æŸå¤±ï¼ˆç™¾åˆ†æ¯”è¯¯å·®ï¼‰
    
    Args:
        pred: (batch, 3, max_dof)
        target: (batch, 3, max_dof)
        dof: (batch,)
    
    Returns:
        loss: æ ‡é‡æŸå¤±
    """
    batch_size = pred.shape[0]
    
    # åˆ›å»ºmask
    mask = torch.zeros_like(pred)
    for i, d in enumerate(dof):
        mask[i, :, :d] = 1.0
    
    # ç›¸å¯¹è¯¯å·®: |pred - target| / (target + eps)
    relative_error = torch.abs(pred - target) / (torch.abs(target) + 1e-2)
    masked_error = relative_error * mask
    
    # å¹³å‡
    num_elements = mask.sum()
    loss = masked_error.sum() / (num_elements + 1e-8)
    
    return loss


def train_epoch(model, dataloader, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    total_mse = 0
    total_rel_error = 0
    
    for features, labels, dofs in dataloader:
        features = features.to(device)
        labels = labels.to(device)  # (batch, 3, max_dof)
        
        # å‰å‘ä¼ æ’­
        kp_pred, ki_pred, kd_pred = model(features)
        pred = torch.stack([kp_pred, ki_pred, kd_pred], dim=1)  # (batch, 3, max_dof)
        
        # è®¡ç®—æŸå¤±
        mse_loss = weighted_mse_loss(pred, labels, dofs)
        rel_loss = relative_error_loss(pred, labels, dofs)
        
        # ç»„åˆæŸå¤±
        loss = mse_loss + 0.1 * rel_loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
        total_mse += mse_loss.item()
        total_rel_error += rel_loss.item()
    
    num_batches = len(dataloader)
    return {
        'loss': total_loss / num_batches,
        'mse': total_mse / num_batches,
        'rel_error': total_rel_error / num_batches
    }


def evaluate(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    total_mse = 0
    total_rel_error = 0
    
    with torch.no_grad():
        for features, labels, dofs in dataloader:
            features = features.to(device)
            labels = labels.to(device)
            
            kp_pred, ki_pred, kd_pred = model(features)
            pred = torch.stack([kp_pred, ki_pred, kd_pred], dim=1)
            
            mse_loss = weighted_mse_loss(pred, labels, dofs)
            rel_loss = relative_error_loss(pred, labels, dofs)
            loss = mse_loss + 0.1 * rel_loss
            
            total_loss += loss.item()
            total_mse += mse_loss.item()
            total_rel_error += rel_loss.item()
    
    num_batches = len(dataloader)
    return {
        'loss': total_loss / num_batches,
        'mse': total_mse / num_batches,
        'rel_error': total_rel_error / num_batches
    }


def plot_training_curves(train_history, val_history, output_path):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    metrics = ['loss', 'mse', 'rel_error']
    titles = ['Total Loss', 'MSE Loss', 'Relative Error']
    
    for ax, metric, title in zip(axes, metrics, titles):
        train_values = [h[metric] for h in train_history]
        val_values = [h[metric] for h in val_history]
        
        ax.plot(train_values, label='Train', linewidth=2)
        ax.plot(val_values, label='Val', linewidth=2)
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel(title, fontsize=12)
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {output_path}")


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("=" * 80)
    print("å…ƒå­¦ä¹ PIDæ¨¡å‹è®­ç»ƒ")
    print("=" * 80)
    
    # é…ç½®
    data_path = Path('meta_learning/training_data')
    dataset_file = list(data_path.glob('pid_dataset_*.json'))
    
    if not dataset_file:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®é›†ï¼")
        print("   è¯·å…ˆè¿è¡Œ: python meta_learning/collect_training_data.py")
        return
    
    dataset_file = dataset_file[-1]  # ä½¿ç”¨æœ€æ–°çš„
    print(f"\nåŠ è½½æ•°æ®é›†: {dataset_file}")
    
    # åŠ è½½æ•°æ®
    with open(dataset_file, 'r') as f:
        data_points = json.load(f)
    
    print(f"æ€»æ•°æ®ç‚¹: {len(data_points)}")
    
    if len(data_points) < 10:
        print("âš ï¸  æ•°æ®ç‚¹å¤ªå°‘ï¼Œå»ºè®®è‡³å°‘20ä¸ªä»¥ä¸Š")
        print("   å½“å‰å°†ä½¿ç”¨ç®€åŒ–è®­ç»ƒæµç¨‹")
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    train_data, val_data = train_test_split(data_points, test_size=0.2, random_state=42)
    print(f"è®­ç»ƒé›†: {len(train_data)}, éªŒè¯é›†: {len(val_data)}")
    
    # åˆ›å»ºæ•°æ®é›†
    feature_extractor = RobotFeatureExtractor()
    train_dataset = PIDDataset(train_data, feature_extractor)
    val_dataset = PIDDataset(val_data, feature_extractor, 
                            normalization_stats=train_dataset.get_normalization_stats())
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    model = MetaPIDNetwork(
        feature_dim=len(feature_extractor.feature_names),
        max_dof=7,
        hidden_dims=[256, 256, 128]
    ).to(device)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10, verbose=True
    )
    
    # è®­ç»ƒ
    num_epochs = 200
    best_val_loss = float('inf')
    train_history = []
    val_history = []
    
    print(f"\nå¼€å§‹è®­ç»ƒ ({num_epochs} epochs)...")
    print("=" * 80)
    
    for epoch in range(num_epochs):
        # è®­ç»ƒ
        train_metrics = train_epoch(model, train_loader, optimizer, device)
        train_history.append(train_metrics)
        
        # éªŒè¯
        val_metrics = evaluate(model, val_loader, device)
        val_history.append(val_metrics)
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step(val_metrics['loss'])
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"Epoch {epoch+1:3d}/{num_epochs} | "
                  f"Train Loss: {train_metrics['loss']:.4f} | "
                  f"Val Loss: {val_metrics['loss']:.4f} | "
                  f"Val MSE: {val_metrics['mse']:.4f} | "
                  f"Val RelErr: {val_metrics['rel_error']:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['loss'] < best_val_loss:
            best_val_loss = val_metrics['loss']
            
            # åˆ›å»ºä¼˜åŒ–å™¨å¹¶ä¿å­˜
            meta_optimizer = MetaPIDOptimizer(device=device)
            meta_optimizer.model = model
            meta_optimizer.normalization_stats = train_dataset.get_normalization_stats()
            
            model_path = Path('meta_learning/models/best_meta_pid.pth')
            model_path.parent.mkdir(parents=True, exist_ok=True)
            meta_optimizer.save(model_path)
            
            if epoch > 0:
                print(f"      ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (val_loss={best_val_loss:.4f})")
    
    print("\n" + "=" * 80)
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_path = Path('meta_learning/models/training_curves.png')
    plot_training_curves(train_history, val_history, plot_path)
    
    print("\nä¸‹ä¸€æ­¥: æµ‹è¯•é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›")
    print("  python meta_learning/test_zero_shot.py")


if __name__ == '__main__':
    main()

