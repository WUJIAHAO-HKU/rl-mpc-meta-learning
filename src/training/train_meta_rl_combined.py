#!/usr/bin/env python3
"""
è®­ç»ƒ Meta-PID + RL ç»„åˆæ§åˆ¶å™¨
ä»å…ƒå­¦ä¹ é¢„æµ‹çš„PIDå¼€å§‹ï¼ŒRLè¿›è¡Œå¾®è°ƒä¼˜åŒ–
"""

import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from pathlib import Path
import os
from meta_rl_combined_env import MetaRLCombinedEnv


def make_env(robot_urdf, rank=0):
    """åˆ›å»ºç¯å¢ƒ"""
    def _init():
        return MetaRLCombinedEnv(
            robot_urdf=robot_urdf,
            gui=False,
            adjustment_range=0.2  # Â±20%è°ƒæ•´èŒƒå›´
        )
    return _init


def train_meta_rl(robot_urdf='franka_panda/panda.urdf', 
                  total_timesteps=1000000,  # ä¼˜åŒ–ï¼šä»200kå¢åŠ åˆ°1M (5å€è®­ç»ƒé‡)
                  n_envs=8,
                  use_gpu=True):
    """
    è®­ç»ƒMeta-PID + RLç»„åˆæ§åˆ¶å™¨
    
    Args:
        robot_urdf: æœºå™¨äººURDFè·¯å¾„
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•° (é»˜è®¤1Mï¼Œå……åˆ†è®­ç»ƒ)
        n_envs: å¹¶è¡Œç¯å¢ƒæ•°
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
    """
    print("=" * 80)
    print("è®­ç»ƒ Meta-PID + RL ç»„åˆæ§åˆ¶å™¨")
    print("=" * 80)
    
    # æ£€æµ‹GPU
    device = 'cuda' if (use_gpu and torch.cuda.is_available()) else 'cpu'
    print(f"\nğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    robot_name = Path(robot_urdf).stem
    log_dir = Path(__file__).parent / 'logs' / f'meta_rl_{robot_name}'
    log_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
    
    # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
    print(f"\nğŸ”§ åˆ›å»º{n_envs}ä¸ªå¹¶è¡Œç¯å¢ƒ...")
    
    if n_envs == 1:
        # å•ç¯å¢ƒ
        env = DummyVecEnv([make_env(robot_urdf, 0)])
    else:
        # å¤šè¿›ç¨‹ç¯å¢ƒ
        env = SubprocVecEnv([make_env(robot_urdf, i) for i in range(n_envs)])
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = DummyVecEnv([make_env(robot_urdf, 999)])
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºPPOæ¨¡å‹ (ä¼˜åŒ–è¶…å‚æ•°é…ç½®)
    print(f"\nğŸ¤– åˆ›å»ºPPOæ¨¡å‹...")
    
    model = PPO(
        'MlpPolicy',
        env,
        learning_rate=1e-4,           # ä¼˜åŒ–ï¼šä»3e-4é™ä½åˆ°1e-4ï¼Œæ›´ç¨³å®šçš„å­¦ä¹ 
        n_steps=2048,                 # ä¼˜åŒ–ï¼šæ¯ä¸ªç¯å¢ƒ2048æ­¥ (æ ‡å‡†PPOé…ç½®)
        batch_size=256,               # ä¼˜åŒ–ï¼šä»64å¢åŠ åˆ°256ï¼Œæ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡
        n_epochs=10,                  # ä¿æŒ10è½®æ›´æ–°
        gamma=0.99,                   # æŠ˜æ‰£å› å­
        gae_lambda=0.95,              # GAE lambda
        clip_range=0.2,               # PPOè£å‰ªèŒƒå›´
        ent_coef=0.02,                # ä¼˜åŒ–ï¼šä»0.01å¢åŠ åˆ°0.02ï¼Œå¢åŠ æ¢ç´¢
        vf_coef=0.5,                  # å€¼å‡½æ•°æŸå¤±ç³»æ•°
        max_grad_norm=0.5,            # æ¢¯åº¦è£å‰ª
        verbose=1,
        device=device,
        tensorboard_log=str(log_dir / 'tensorboard')
    )
    
    print(f"âœ… PPOæ¨¡å‹åˆ›å»ºæˆåŠŸ (ä¼˜åŒ–é…ç½®)")
    print(f"   ç­–ç•¥ç½‘ç»œ: MlpPolicy")
    print(f"   å­¦ä¹ ç‡: 1e-4 (é™ä½ä»¥æé«˜ç¨³å®šæ€§)")
    print(f"   Steps per env: 2048 (æ ‡å‡†PPOé…ç½®)")
    print(f"   æ‰¹æ¬¡å¤§å°: 256 (å¢å¤§ä»¥æé«˜ç¨³å®šæ€§)")
    print(f"   ç†µç³»æ•°: 0.02 (å¢åŠ æ¢ç´¢)")
    print(f"   æ€»æ”¶é›†æ­¥æ•°: {2048 * n_envs} per rollout")
    
    # è®¾ç½®å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(log_dir / 'best_model'),
        log_path=str(log_dir / 'eval_logs'),
        eval_freq=10000,
        n_eval_episodes=5,
        deterministic=True,
        render=False
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=20000,
        save_path=str(log_dir / 'checkpoints'),
        name_prefix='meta_rl'
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"{'='*80}")
    print(f"   æ€»æ­¥æ•°: {total_timesteps:,}")
    print(f"   å¹¶è¡Œç¯å¢ƒ: {n_envs}")
    print(f"   æ¯è½®æ”¶é›†: {2048 * n_envs:,} æ­¥")
    print(f"   è¯„ä¼°é¢‘ç‡: æ¯10,000æ­¥")
    print(f"   æ£€æŸ¥ç‚¹: æ¯20,000æ­¥")
    
    # ä¼°ç®—è®­ç»ƒæ—¶é—´
    if total_timesteps <= 200000:
        est_time = "~30-45åˆ†é’Ÿ"
    elif total_timesteps <= 500000:
        est_time = "~1.5-2.5å°æ—¶"
    elif total_timesteps <= 1000000:
        est_time = "~3-5å°æ—¶"
    else:
        est_time = "~5-10å°æ—¶"
    
    print(f"\nâ° é¢„è®¡è®­ç»ƒæ—¶é—´: {est_time}")
    print(f"   (å–å†³äºç¡¬ä»¶æ€§èƒ½ï¼ŒGPUå¯æ˜¾è‘—åŠ é€Ÿ)")
    print(f"{'='*80}\n")
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=[eval_callback, checkpoint_callback],
            progress_bar=True
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = log_dir / f'meta_rl_{robot_name}_final.zip'
        model.save(final_model_path)
        
        print(f"\n{'='*80}")
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"{'='*80}")
        print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®:")
        print(f"   æœ€ç»ˆæ¨¡å‹: {final_model_path}")
        print(f"   æœ€ä½³æ¨¡å‹: {log_dir / 'best_model' / 'best_model.zip'}")
        print(f"   æ£€æŸ¥ç‚¹: {log_dir / 'checkpoints'}")
        print(f"\nğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿:")
        print(f"   tensorboard --logdir {log_dir / 'tensorboard'}")
        print(f"{'='*80}")
        
    except KeyboardInterrupt:
        print(f"\nâ¸ï¸  è®­ç»ƒè¢«ä¸­æ–­")
    finally:
        # æ¸…ç†èµ„æº
        print(f"\nğŸ§¹ æ¸…ç†èµ„æº...")
        env.close()
        eval_env.close()
        if device == 'cuda':
            torch.cuda.empty_cache()
        print(f"   âœ… èµ„æºæ¸…ç†å®Œæˆ")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
if __name__ == '__main__':
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    robot_urdf = sys.argv[1] if len(sys.argv) > 1 else 'franka_panda/panda.urdf'
    total_timesteps = int(sys.argv[2]) if len(sys.argv) > 2 else 1000000  # ä¼˜åŒ–ï¼šé»˜è®¤1Mæ­¥
    n_envs = int(sys.argv[3]) if len(sys.argv) > 3 else 8
    
    print("=" * 80)
    print("ğŸ¯ ä¼˜åŒ–ç‰ˆæœ¬ RLè®­ç»ƒè„šæœ¬")
    print("=" * 80)
    print(f"ğŸ“ˆ å…³é”®æ”¹è¿›:")
    print(f"   â€¢ è®­ç»ƒæ­¥æ•°: 200k â†’ 1M (5å€)")
    print(f"   â€¢ å­¦ä¹ ç‡: 3e-4 â†’ 1e-4 (æ›´ç¨³å®š)")
    print(f"   â€¢ Steps/env: 256 â†’ 2048 (8å€)")
    print(f"   â€¢ Batch size: 64 â†’ 256 (4å€)")
    print(f"   â€¢ ç†µç³»æ•°: 0.01 â†’ 0.02 (æ›´å¤šæ¢ç´¢)")
    print(f"=" * 80)
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   python {sys.argv[0]} [robot_urdf] [total_steps] [n_envs]")
    print(f"\nğŸ“ ç¤ºä¾‹:")
    print(f"   python {sys.argv[0]} franka_panda/panda.urdf 1000000 8")
    print(f"   python {sys.argv[0]} laikago/laikago.urdf 1000000 8")
    print(f"=" * 80)
    print(f"\n")
    
    # è®­ç»ƒ
    train_meta_rl(
        robot_urdf=robot_urdf,
        total_timesteps=total_timesteps,
        n_envs=n_envs,
        use_gpu=True
    )

