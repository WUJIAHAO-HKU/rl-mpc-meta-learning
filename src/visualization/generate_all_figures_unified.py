#!/usr/bin/env python3
"""
Áªü‰∏ÄÁîüÊàêÊâÄÊúâÂõæË°®ÂíåË°®Ê†ºÔºåÁ°Æ‰øùÊï∞ÊçÆ‰∏ÄËá¥ÊÄß
ÂêåÊó∂ÁîüÊàêÔºö
1. Figure 3: Per-joint comparison (‰∏§‰∏™Êú∫Âô®‰∫∫Âπ≥Âè∞)
2. Figure 4: Comprehensive tracking performance (Franka PandaËØ¶ÁªÜÂàÜÊûê)
3. LaTeXË°®Ê†ºÊï∞ÊçÆ
"""

import numpy as np
import pybullet as p
import torch
from stable_baselines3 import PPO
from meta_rl_combined_env import MetaRLCombinedEnv
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from matplotlib.patches import Rectangle
import json


def setup_publication_style():
    """ËÆæÁΩÆÂá∫ÁâàÁ∫ßÂà´ÁöÑÂõæË°®Ê†∑Âºè"""
    plt.rcParams.update({
        'font.family': 'serif',
        'font.serif': ['Times New Roman'],
        'font.size': 10,
        'axes.labelsize': 11,
        'axes.titlesize': 12,
        'xtick.labelsize': 9,
        'ytick.labelsize': 9,
        'legend.fontsize': 9,
        'figure.titlesize': 13,
        'axes.linewidth': 1.0,
        'grid.linewidth': 0.5,
        'lines.linewidth': 1.5,
        'patch.linewidth': 0.5,
        'xtick.major.width': 1.0,
        'ytick.major.width': 1.0,
        'axes.grid': False,
        'grid.alpha': 0.3,
        'figure.dpi': 100,
        'savefig.dpi': 300,
        'savefig.bbox': 'tight',
    })


def evaluate_tracking_performance(robot_urdf, model_path=None, steps=10000, test_name=""):
    """
    ËØÑ‰º∞Ë∑üË∏™ÊÄßËÉΩ
    """
    print(f"\n{'='*80}")
    print(f"ËØÑ‰º∞: {test_name}")
    print(f"{'='*80}")
    
    # ÂàõÂª∫ÁéØÂ¢É
    env = MetaRLCombinedEnv(robot_urdf=robot_urdf, gui=False)
    
    # Âä†ËΩΩRLÊ®°ÂûãÔºàÂ¶ÇÊûúÊúâÔºâ
    model = None
    if model_path is not None:
        try:
            model = PPO.load(model_path)
            print(f"‚úÖ RLÊ®°ÂûãÂä†ËΩΩÊàêÂäü")
        except Exception as e:
            print(f"‚ö†Ô∏è  RLÊ®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {e}")
            print(f"   ‰ΩøÁî®Âõ∫ÂÆöMeta-PID")
    else:
        print(f"‚úÖ ‰ΩøÁî®Âõ∫ÂÆöMeta-PIDÔºàÊó†RLË∞ÉÊï¥Ôºâ")
    
    obs, _ = env.reset()
    
    # ËÆ∞ÂΩïÊï∞ÊçÆ
    actual_errors_deg = []  # ÊÄªËØØÂ∑Æ (ËßíÂ∫¶)
    joint_errors = []  # ÊØè‰∏™ÂÖ≥ËäÇÁöÑËØØÂ∑Æ (ÂºßÂ∫¶)
    
    for step in range(steps):
        # ÈÄâÊã©Âä®‰Ωú
        if model is not None:
            action, _ = model.predict(obs, deterministic=True)
        else:
            action = np.zeros(2)  # Âõ∫ÂÆöMeta-PID
        
        # ÊâßË°åÂä®‰Ωú
        obs, reward, terminated, truncated, info = env.step(action)
        
        # Ëé∑ÂèñÂÆûÈôÖÂÖ≥ËäÇËØØÂ∑Æ
        joint_states = p.getJointStates(env.robot_id, env.controllable_joints)
        q_actual = np.array([s[0] for s in joint_states])
        q_ref = env._get_reference_trajectory()
        
        # ËÆ°ÁÆóÂÆûÈôÖËØØÂ∑Æ
        joint_error = np.abs(q_ref - q_actual)  # ÊØè‰∏™ÂÖ≥ËäÇÁöÑÁªùÂØπËØØÂ∑ÆÔºàÂºßÂ∫¶Ôºâ
        actual_error_rad = np.linalg.norm(q_ref - q_actual)  # ÊÄªËØØÂ∑ÆËåÉÊï∞ÔºàÂºßÂ∫¶Ôºâ
        actual_error_deg = np.degrees(actual_error_rad)  # ËΩ¨Êç¢‰∏∫ËßíÂ∫¶
        
        actual_errors_deg.append(actual_error_deg)
        joint_errors.append(joint_error)
        
        if step % 2000 == 0:
            print(f"Step {step:5d}: error={actual_error_deg:.2f}¬∞")
        
        if terminated or truncated:
            obs, _ = env.reset()
    
    env.close()
    
    # ËΩ¨Êç¢‰∏∫numpyÊï∞ÁªÑ
    actual_errors_deg = np.array(actual_errors_deg)
    joint_errors = np.array(joint_errors)  # shape: (steps, n_joints)
    
    # ËÆ°ÁÆóÁªüËÆ°Èáè
    overall_mae = np.mean(actual_errors_deg)
    overall_rmse = np.sqrt(np.mean(actual_errors_deg**2))
    overall_max = np.max(actual_errors_deg)
    
    # ËÆ°ÁÆóÊØè‰∏™ÂÖ≥ËäÇÁöÑÁªüËÆ°Èáè
    joint_errors_deg = np.degrees(joint_errors)
    per_joint_mean = np.mean(joint_errors_deg, axis=0)
    per_joint_std = np.std(joint_errors_deg, axis=0)
    
    results = {
        'actual_errors_deg': actual_errors_deg,
        'joint_errors': joint_errors,  # ÂºßÂ∫¶
        'overall_mae': overall_mae,
        'overall_rmse': overall_rmse,
        'overall_max': overall_max,
        'per_joint_mean': per_joint_mean,
        'per_joint_std': per_joint_std,
        'n_joints': len(per_joint_mean)
    }
    
    print(f"\nüìä {test_name} ÁªìÊûú:")
    print(f"   ÊÄª‰ΩìMAE: {overall_mae:.2f}¬∞")
    print(f"   ÊÄª‰ΩìRMSE: {overall_rmse:.2f}¬∞")
    print(f"   ÊúÄÂ§ßËØØÂ∑Æ: {overall_max:.2f}¬∞")
    print(f"\n   ÂêÑÂÖ≥ËäÇÂπ≥ÂùáËØØÂ∑Æ:")
    for i, (mean_err, std_err) in enumerate(zip(per_joint_mean, per_joint_std)):
        print(f"      ÂÖ≥ËäÇ {i+1:2d}: {mean_err:6.2f}¬∞ ¬± {std_err:5.2f}¬∞")
    
    return results


def generate_figure3(all_results, save_path='per_joint_error.png'):
    """
    ÁîüÊàêFigure 3: Per-joint tracking error comparison across platforms
    """
    setup_publication_style()
    
    n_robots = len(all_results)
    fig, axes = plt.subplots(1, n_robots, figsize=(7*n_robots, 5))
    
    if n_robots == 1:
        axes = [axes]
    
    for idx, (robot_name, data) in enumerate(all_results.items()):
        ax = axes[idx]
        
        pure_results = data['pure']
        rl_results = data['rl']
        
        n_joints = pure_results['n_joints']
        joint_indices = np.arange(1, n_joints + 1)
        
        pure_mean = pure_results['per_joint_mean']
        rl_mean = rl_results['per_joint_mean']
        pure_std = pure_results['per_joint_std']
        rl_std = rl_results['per_joint_std']
        
        width = 0.35
        x = joint_indices
        
        # ÁªòÂà∂Êü±Áä∂Âõæ
        bars1 = ax.bar(x - width/2, pure_mean, width, 
                       yerr=pure_std, capsize=3,
                       label='Pure Meta-PID', 
                       color='steelblue', alpha=0.8, edgecolor='black', linewidth=0.5)
        bars2 = ax.bar(x + width/2, rl_mean, width,
                       yerr=rl_std, capsize=3,
                       label='Meta-PID + RL', 
                       color='coral', alpha=0.8, edgecolor='black', linewidth=0.5)
        
        # Âú®Êü±Â≠ê‰∏äÊñπÊ†áÊ≥®ÊîπÂñÑÁôæÂàÜÊØî
        for i, (p_err, r_err) in enumerate(zip(pure_mean, rl_mean)):
            improvement = (p_err - r_err) / p_err * 100
            y_pos = max(p_err, r_err) + max(pure_std[i], rl_std[i])
            if abs(improvement) > 1:  # Âè™ÊòæÁ§∫ÊîπÂñÑË∂ÖËøá1%ÁöÑ
                color = 'green' if improvement > 0 else 'red'
                ax.text(i + 1, y_pos, f'{improvement:+.1f}%', 
                       ha='center', va='bottom', fontsize=8, color=color, fontweight='bold')
        
        ax.set_xlabel('Joint Index', fontsize=12, fontweight='bold')
        ax.set_ylabel('Mean Absolute Error (degrees)', fontsize=12, fontweight='bold')
        ax.set_title(f'{robot_name} ({n_joints}-DOF)', fontsize=14, fontweight='bold')
        ax.set_xticks(joint_indices)
        ax.set_ylim(0, None)  # YËΩ¥‰ªé0ÂºÄÂßãÔºåÂíåÂ≠êÂõæb‰øùÊåÅ‰∏ÄËá¥
        ax.legend(loc='upper right', fontsize=10)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        
        # Ê∑ªÂä†ÊÄª‰ΩìMAEÊ†áÊ≥®
        pure_overall = pure_results['overall_mae']
        rl_overall = rl_results['overall_mae']
        overall_improvement = (pure_overall - rl_overall) / pure_overall * 100
        
        ax.text(0.02, 0.98, f'Overall: {overall_improvement:+.1f}%', 
               transform=ax.transAxes, ha='left', va='top',
               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.6),
               fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ Figure 3Â∑≤‰øùÂ≠ò: {save_path}")
    plt.close()


def generate_figure4(pure_results, rl_results, save_path='Figure4_comprehensive_tracking_performance.png'):
    """
    ÁîüÊàêFigure 4: Comprehensive tracking performance (Franka PandaËØ¶ÁªÜÂàÜÊûê)
    """
    setup_publication_style()
    
    # ÂàõÂª∫2x2Â≠êÂõæÂ∏ÉÂ±Ä
    fig = plt.figure(figsize=(14, 10))
    gs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
    
    # È¢úËâ≤ÊñπÊ°à
    color_pure = '#4A90E2'  # ËìùËâ≤
    color_rl = '#F5A623'    # Ê©ôËâ≤
    
    # ========================================================================
    # Â≠êÂõæ (a): Actual Tracking Error Comparison
    # ========================================================================
    ax1 = fig.add_subplot(gs[0, 0])
    
    # Âπ≥ÊªëÂ§ÑÁêÜ
    window = 100
    pure_smooth = np.convolve(pure_results['actual_errors_deg'], 
                              np.ones(window)/window, mode='valid')
    rl_smooth = np.convolve(rl_results['actual_errors_deg'], 
                            np.ones(window)/window, mode='valid')
    
    ax1.plot(pure_smooth, label='Pure Meta-PID', color=color_pure, alpha=0.8, linewidth=1.5)
    ax1.plot(rl_smooth, label='Meta-PID + RL', color=color_rl, alpha=0.8, linewidth=1.5)
    ax1.set_xlabel('Time Step', fontweight='bold')
    ax1.set_ylabel('Tracking Error (degrees)', fontweight='bold')
    ax1.set_title('(a) Actual Tracking Error Comparison', fontweight='bold', loc='left')
    ax1.legend(loc='upper right', framealpha=0.9)
    ax1.grid(True, alpha=0.3, linestyle='--')
    
    # ËÆ°ÁÆóÊîπÂñÑÁôæÂàÜÊØî
    improvement = (pure_results['overall_mae'] - rl_results['overall_mae']) / pure_results['overall_mae'] * 100
    ax1.text(0.98, 0.02, f'{improvement:.1f}% improvement with RL adaptation', 
             transform=ax1.transAxes, ha='right', va='bottom',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
             fontsize=9, fontweight='bold')
    
    # ========================================================================
    # Â≠êÂõæ (b): Error Distribution
    # ========================================================================
    ax2 = fig.add_subplot(gs[0, 1])
    
    ax2.hist(pure_results['actual_errors_deg'], bins=50, alpha=0.6, 
            color=color_pure, label='Pure Meta-PID', density=True, edgecolor='black', linewidth=0.5)
    ax2.hist(rl_results['actual_errors_deg'], bins=50, alpha=0.6, 
            color=color_rl, label='Meta-PID + RL', density=True, edgecolor='black', linewidth=0.5)
    ax2.set_xlabel('Tracking Error (degrees)', fontweight='bold')
    ax2.set_ylabel('Density', fontweight='bold')
    ax2.set_title('(b) Error Distribution', fontweight='bold', loc='left')
    ax2.legend(loc='upper right', framealpha=0.9)
    ax2.grid(True, alpha=0.3, linestyle='--', axis='y')
    
    # Ê∑ªÂä†ÂùáÂÄºÁ∫ø
    ax2.axvline(pure_results['overall_mae'], color=color_pure, linestyle='--', linewidth=2, alpha=0.7)
    ax2.axvline(rl_results['overall_mae'], color=color_rl, linestyle='--', linewidth=2, alpha=0.7)
    
    # ========================================================================
    # Â≠êÂõæ (c): Per-Joint Error Comparison with Improvement Curve (ÂèåYËΩ¥)
    # ========================================================================
    ax3 = fig.add_subplot(gs[1, 0])
    
    # ËÆ°ÁÆóÂêÑÂÖ≥ËäÇÂπ≥ÂùáËØØÂ∑Æ
    mean_joint_errors_pure = np.mean(pure_results['joint_errors'], axis=0)
    mean_joint_errors_rl = np.mean(rl_results['joint_errors'], axis=0)
    
    n_joints = len(mean_joint_errors_pure)
    x = np.arange(n_joints) + 1  # Joint indices starting from 1
    width = 0.35
    
    # Â∑¶YËΩ¥ÔºöËØØÂ∑ÆÂÄºÊü±Áä∂Âõæ
    bars1 = ax3.bar(x - width/2, np.degrees(mean_joint_errors_pure), width, 
                     label='Pure Meta-PID', color=color_pure, alpha=0.8, edgecolor='black', linewidth=0.5)
    bars2 = ax3.bar(x + width/2, np.degrees(mean_joint_errors_rl), width, 
                     label='Meta-PID + RL', color=color_rl, alpha=0.8, edgecolor='black', linewidth=0.5)
    
    ax3.set_xlabel('Joint Index', fontweight='bold')
    ax3.set_ylabel('Mean Absolute Error (degrees)', fontweight='bold', color='black')
    ax3.set_title('(c) Per-Joint Error Comparison', fontweight='bold', loc='left')
    ax3.set_xticks(x)
    ax3.set_xticklabels([f'J{i}' for i in x])
    ax3.tick_params(axis='y', labelcolor='black')
    ax3.grid(True, alpha=0.3, linestyle='--', axis='y')
    
    # ÂàõÂª∫Âè≥YËΩ¥ÔºöÊîπËøõÁôæÂàÜÊØîÊõ≤Á∫ø
    ax3_twin = ax3.twinx()
    
    # ËÆ°ÁÆóÊØè‰∏™ÂÖ≥ËäÇÁöÑÊîπËøõÁôæÂàÜÊØî
    improvement_percentages = []
    for i in range(n_joints):
        pure_err = np.degrees(mean_joint_errors_pure[i])
        rl_err = np.degrees(mean_joint_errors_rl[i])
        if pure_err > 0:
            improvement_pct = (pure_err - rl_err) / pure_err * 100
        else:
            improvement_pct = 0
        improvement_percentages.append(improvement_pct)
    
    improvement_percentages = np.array(improvement_percentages)
    
    # ÁªòÂà∂ÊîπËøõÁôæÂàÜÊØîÊõ≤Á∫øÔºà‰ΩøÁî®Ê∑±ÁªøËâ≤Ôºâ
    color_improvement = '#2E7D32'  # Ê∑±ÁªøËâ≤
    line = ax3_twin.plot(x, improvement_percentages, 
                         color=color_improvement, marker='o', markersize=6,
                         linewidth=2.5, label='Improvement (%)', 
                         linestyle='-', alpha=0.9, zorder=10)
    
    # Âú®Êï∞ÊçÆÁÇπ‰∏äÊ†áÊ≥®ÊîπÂñÑÁôæÂàÜÊØîÔºàJ2ÊîæÂú®‰∏äÊñπÔºåÂÖ∂‰ªñÊîæÂú®‰∏ãÊñπÔºâ
    for i, (xi, yi) in enumerate(zip(x, improvement_percentages)):
        if abs(yi) > 1:  # Âè™ÊòæÁ§∫ÊîπÂñÑË∂ÖËøá1%ÁöÑ
            color_text = 'green' if yi > 0 else 'red'
            
            # J2Ôºài=1ÔºåÂõ†‰∏∫Á¥¢Âºï‰ªé0ÂºÄÂßãÔºâÊîæÂú®Êõ≤Á∫ø‰∏äÊñπÔºåÂÖ∂‰ªñÊîæÂú®‰∏ãÊñπ
            if i == 1:  # J2
                y_offset = yi + 2.5
                va = 'bottom'
            else:  # ÂÖ∂‰ªñÂÖ≥ËäÇ
                y_offset = yi - 3.0
                va = 'top'
            
            ax3_twin.text(xi, y_offset, f'{yi:+.1f}%', 
                         ha='center', va=va, fontsize=7, 
                         color=color_text, fontweight='bold',
                         bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                                 edgecolor=color_text, alpha=0.7, linewidth=1))
    
    ax3_twin.set_ylabel('Improvement (%)', fontweight='bold', color=color_improvement)
    ax3_twin.tick_params(axis='y', labelcolor=color_improvement)
    ax3_twin.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5, zorder=1)
    
    # ËÆæÁΩÆÂè≥YËΩ¥ËåÉÂõ¥Ôºà‰∏∫‰∏ãÊñπÊ†áÊ≥®ÁïôÂá∫Êõ¥Â§öÁ©∫Èó¥Ôºâ
    max_abs_improvement = max(abs(improvement_percentages.min()), abs(improvement_percentages.max()))
    ax3_twin.set_ylim(-max_abs_improvement * 0.5, max_abs_improvement * 1.3)
    
    # ÂêàÂπ∂Âõæ‰æãÔºàÊîæÂú®‰∏≠Èó¥‰∏äÊñπÔºåÈÅøÂÖçÈÅÆÊå°Êï∞ÊçÆÔºâ
    lines1, labels1 = ax3.get_legend_handles_labels()
    lines2, labels2 = ax3_twin.get_legend_handles_labels()
    ax3.legend(lines1 + lines2, labels1 + labels2, 
              loc='upper center',           # ‰ΩçÁΩÆÔºö‰∏äÊñπ‰∏≠Èó¥
              bbox_to_anchor=(0.5, 0.8),   # Á≤æÁ°Æ‰ΩçÁΩÆÔºöÊ∞¥Âπ≥‰∏≠ÂøÉ(0.5), ÂõæË°®ÂÜÖÈÉ®‰∏äÊñπ
              framealpha=0.95,              # ËÉåÊôØÈÄèÊòéÂ∫¶
              fontsize=8,                   # Â≠ó‰ΩìÂ§ßÂ∞è
              edgecolor='gray',             # ËæπÊ°ÜÈ¢úËâ≤
              fancybox=True)                # ÂúÜËßíËæπÊ°Ü
    
    # Ê∑ªÂä†ÊîπÂñÑ‰ø°ÊÅØÊñáÊú¨Ê°Ü
    joints_improved = np.sum(improvement_percentages > 0)
    avg_joint_improvement = np.mean(improvement_percentages[improvement_percentages > 0]) if joints_improved > 0 else 0
    max_improvement_joint = np.argmax(improvement_percentages) + 1
    max_improvement_value = improvement_percentages[np.argmax(improvement_percentages)]
    
    info_text = f'Joint {max_improvement_joint} benefits most: {max_improvement_value:.1f}% improvement\n{joints_improved}/{n_joints} joints improved, avg {avg_joint_improvement:.1f}%'
    ax3.text(0.98, 0.98, info_text,
             transform=ax3.transAxes, ha='right', va='top',
             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.6, edgecolor='darkgreen'),
             fontsize=7, fontweight='bold')
    
    # ========================================================================
    # Â≠êÂõæ (d): Cumulative Distribution Function
    # ========================================================================
    ax4 = fig.add_subplot(gs[1, 1])
    
    pure_sorted = np.sort(pure_results['actual_errors_deg'])
    rl_sorted = np.sort(rl_results['actual_errors_deg'])
    pure_cdf = np.arange(1, len(pure_sorted)+1) / len(pure_sorted)
    rl_cdf = np.arange(1, len(rl_sorted)+1) / len(rl_sorted)
    
    ax4.plot(pure_sorted, pure_cdf, label='Pure Meta-PID', color=color_pure, linewidth=2, alpha=0.8)
    ax4.plot(rl_sorted, rl_cdf, label='Meta-PID + RL', color=color_rl, linewidth=2, alpha=0.8)
    ax4.set_xlabel('Tracking Error (degrees)', fontweight='bold')
    ax4.set_ylabel('Cumulative Probability', fontweight='bold')
    ax4.set_title('(d) Cumulative Distribution Function', fontweight='bold', loc='left')
    ax4.legend(loc='lower right', framealpha=0.9)
    ax4.grid(True, alpha=0.3, linestyle='--')
    
    # Ê†áÊ≥®ÂÖ≥ÈîÆÁôæÂàÜ‰ΩçÊï∞ÁöÑÊîπÂñÑ
    percentiles = [50, 90]
    for pct in percentiles:
        idx = int(len(pure_sorted) * pct / 100)
        pure_val = pure_sorted[idx]
        rl_val = rl_sorted[idx]
        improvement = (pure_val - rl_val) / pure_val * 100
        ax4.axhline(pct/100, color='gray', linestyle=':', alpha=0.3)
        ax4.text(0.98, pct/100 - 0.05, f'P{pct}: {improvement:+.1f}%', 
                transform=ax4.transAxes, ha='right', va='bottom',
                fontsize=8, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ Figure 4Â∑≤‰øùÂ≠ò: {save_path}")
    plt.close()


def generate_latex_table(all_results, save_path='per_joint_error_table.tex'):
    """
    ÁîüÊàêLaTeXË°®Ê†º
    """
    print("\n" + "="*80)
    print("LaTeXË°®Ê†º‰ª£Á†Å")
    print("="*80)
    
    latex_code = """\\begin{table*}[!htbp]
\\caption{Per-Joint Tracking Error Comparison Across Platforms}
\\label{tab:per_joint_error}
\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lllll@{}}
\\toprule
\\textbf{Robot} & \\textbf{Joint} & \\textbf{Pure Meta-PID (¬∞)} & \\textbf{Meta-PID+RL (¬∞)} & \\textbf{Improv.} \\\\
\\midrule
"""
    
    for robot_name, data in all_results.items():
        pure_results = data['pure']
        rl_results = data['rl']
        n_joints = pure_results['n_joints']
        
        for i in range(n_joints):
            pure_err = pure_results['per_joint_mean'][i]
            rl_err = rl_results['per_joint_mean'][i]
            improvement = (pure_err - rl_err) / pure_err * 100
            
            robot_col = robot_name if i == 0 else ""
            joint_col = f"J{i+1}"
            
            latex_code += f"{robot_col:<15} & {joint_col:<6} & {pure_err:6.2f} & {rl_err:6.2f} & {improvement:+5.1f}\\% \\\\\n"
        
        # Ê∑ªÂä†ÊÄª‰ΩìÁªüËÆ°
        pure_mae = pure_results['overall_mae']
        rl_mae = rl_results['overall_mae']
        overall_improvement = (pure_mae - rl_mae) / pure_mae * 100
        
        latex_code += f"\\midrule\n"
        latex_code += f"\\textit{{{robot_name} Avg}} & & {pure_mae:6.2f} & {rl_mae:6.2f} & {overall_improvement:+5.1f}\\% \\\\\n"
        
        if robot_name != list(all_results.keys())[-1]:
            latex_code += "\\midrule\n"
    
    latex_code += """\\bottomrule
\\end{tabular*}
\\end{table*}
"""
    
    print(latex_code)
    
    # ‰øùÂ≠òÂà∞Êñá‰ª∂
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(latex_code)
    print(f"‚úÖ LaTeXË°®Ê†ºÂ∑≤‰øùÂ≠ò: {save_path}")
    
    return latex_code


def save_results_json(all_results, save_path='evaluation_results.json'):
    """
    ‰øùÂ≠òËØÑ‰º∞ÁªìÊûú‰∏∫JSONÔºàÊñπ‰æøÂêéÁª≠‰ΩøÁî®Ôºâ
    """
    # ËΩ¨Êç¢numpyÊï∞ÁªÑ‰∏∫ÂàóË°®
    results_dict = {}
    for robot_name, data in all_results.items():
        results_dict[robot_name] = {}
        for method in ['pure', 'rl']:
            results_dict[robot_name][method] = {
                'overall_mae': float(data[method]['overall_mae']),
                'overall_rmse': float(data[method]['overall_rmse']),
                'overall_max': float(data[method]['overall_max']),
                'per_joint_mean': data[method]['per_joint_mean'].tolist(),
                'per_joint_std': data[method]['per_joint_std'].tolist(),
                'n_joints': int(data[method]['n_joints'])
            }
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2)
    
    print(f"‚úÖ ËØÑ‰º∞ÁªìÊûúÂ∑≤‰øùÂ≠ò‰∏∫JSON: {save_path}")


def main():
    """‰∏ªÂáΩÊï∞"""
    
    # ÂÆö‰πâË¶ÅËØÑ‰º∞ÁöÑÊú∫Âô®‰∫∫
    robots = [
        {
            'urdf': 'franka_panda/panda.urdf',
            'name': 'Franka Panda',
            'rl_model': 'logs/meta_rl_panda/best_model/best_model'
        },
        {
            'urdf': 'laikago/laikago.urdf',
            'name': 'Laikago',
            'rl_model': 'logs/meta_rl_laikago/best_model/best_model'
        }
    ]
    
    print("="*80)
    print("Áªü‰∏ÄËØÑ‰º∞ÔºöÁîüÊàêÊâÄÊúâÂõæË°®ÂíåË°®Ê†º")
    print("="*80)
    print(f"ÊµãËØïÊ≠•Êï∞: 10000")
    print(f"ËØÑ‰º∞Âπ≥Âè∞: {len(robots)} ‰∏™")
    print()
    
    # Â≠òÂÇ®ÊâÄÊúâÁªìÊûú
    all_results = {}
    
    for robot in robots:
        robot_name = robot['name']
        robot_urdf = robot['urdf']
        rl_model_path = robot['rl_model']
        
        print(f"\n{'='*80}")
        print(f"ÂºÄÂßãËØÑ‰º∞: {robot_name}")
        print(f"{'='*80}")
        
        # ËØÑ‰º∞Á∫ØMeta-PID
        pure_results = evaluate_tracking_performance(
            robot_urdf=robot_urdf,
            model_path=None,
            steps=10000,
            test_name=f"{robot_name} - Pure Meta-PID"
        )
        
        # ËØÑ‰º∞Meta-PID + RL
        rl_results = evaluate_tracking_performance(
            robot_urdf=robot_urdf,
            model_path=rl_model_path,
            steps=10000,
            test_name=f"{robot_name} - Meta-PID + RL"
        )
        
        all_results[robot_name] = {
            'pure': pure_results,
            'rl': rl_results
        }
    
    # ÁîüÊàêÊâÄÊúâÂõæË°®ÂíåË°®Ê†º
    print("\n" + "="*80)
    print("ÁîüÊàêÂõæË°®ÂíåË°®Ê†º")
    print("="*80)
    
    # Figure 3: Per-joint comparison
    generate_figure3(all_results, save_path='per_joint_error.png')
    
    # Figure 4: Comprehensive tracking (Franka Panda only)
    generate_figure4(
        all_results['Franka Panda']['pure'],
        all_results['Franka Panda']['rl'],
        save_path='Figure4_comprehensive_tracking_performance.png'
    )
    
    # LaTeXË°®Ê†º
    generate_latex_table(all_results, save_path='per_joint_error_table.tex')
    
    # ‰øùÂ≠òJSONÁªìÊûú
    save_results_json(all_results, save_path='evaluation_results.json')
    
    # ÊâìÂç∞Ê±áÊÄª
    print("\n" + "="*80)
    print("‚úÖ ÊâÄÊúâËØÑ‰º∞ÂÆåÊàêÔºÅ")
    print("="*80)
    print("\nÁîüÊàêÁöÑÊñá‰ª∂:")
    print("  1. per_joint_error.png  (Figure 3)")
    print("  2. Figure4_comprehensive_tracking_performance.png  (Figure 4)")
    print("  3. per_joint_error_table.tex  (LaTeXË°®Ê†º)")
    print("  4. evaluation_results.json  (ËØÑ‰º∞ÁªìÊûúÊï∞ÊçÆ)")
    print("\nüìä ÂÖ≥ÈîÆÁªìÊûú:")
    for robot_name, data in all_results.items():
        pure_mae = data['pure']['overall_mae']
        rl_mae = data['rl']['overall_mae']
        improvement = (pure_mae - rl_mae) / pure_mae * 100
        print(f"  {robot_name}: {improvement:+.1f}% improvement")


if __name__ == '__main__':
    main()

