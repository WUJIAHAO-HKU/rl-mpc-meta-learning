#!/usr/bin/env python3
"""
è¯„ä¼° Meta-PID + RL ç»„åˆæ§åˆ¶å™¨
å¯¹æ¯”çº¯Meta-PIDå’ŒMeta-PID+RLçš„æ€§èƒ½
"""

import numpy as np
import pybullet as p
import torch
from stable_baselines3 import PPO
from meta_rl_combined_env import MetaRLCombinedEnv
import matplotlib.pyplot as plt
from pathlib import Path


def evaluate_pure_meta_pid(robot_urdf, steps=10000):
    """è¯„ä¼°çº¯Meta-PIDï¼ˆå›ºå®šé¢„æµ‹çš„PIDï¼‰"""
    print("\n" + "="*80)
    print("è¯„ä¼° 1: çº¯Meta-PIDï¼ˆå›ºå®šé¢„æµ‹å€¼ï¼‰")
    print("="*80)
    
    # åˆ›å»ºç¯å¢ƒ
    env = MetaRLCombinedEnv(robot_urdf=robot_urdf, gui=False)
    
    obs, _ = env.reset()
    
    # è®°å½•æ•°æ®
    errors = []
    kp_values = []
    kd_values = []
    rewards = []
    
    # ä½¿ç”¨é›¶åŠ¨ä½œï¼ˆä¸è°ƒæ•´ï¼Œä¿æŒMeta-PIDé¢„æµ‹å€¼ï¼‰
    zero_action = np.zeros(2)
    
    for step in range(steps):
        obs, reward, terminated, truncated, info = env.step(zero_action)
        
        errors.append(info['tracking_error'])
        kp_values.append(info['current_kp'])
        kd_values.append(info['current_kd'])
        rewards.append(reward)
        
        if step % 2000 == 0:
            print(f"Step {step:5d}: error={info['tracking_error']:.4f}, "
                  f"Kp={info['current_kp']:.2f}, Kd={info['current_kd']:.2f}")
        
        if terminated or truncated:
            obs, _ = env.reset()
    
    env.close()
    
    results = {
        'errors': np.array(errors),
        'kp_values': np.array(kp_values),
        'kd_values': np.array(kd_values),
        'rewards': np.array(rewards),
        'total_reward': np.sum(rewards),
        'mean_error': np.mean(errors),
        'median_error': np.median(errors),
        'max_error': np.max(errors)
    }
    
    print(f"\nçº¯Meta-PID æ€»å¥–åŠ±: {results['total_reward']:.2f}")
    print(f"çº¯Meta-PID å¹³å‡è¯¯å·®: {results['mean_error']:.4f}")
    print(f"çº¯Meta-PID ä¸­ä½è¯¯å·®: {results['median_error']:.4f}")
    print(f"çº¯Meta-PID æœ€å¤§è¯¯å·®: {results['max_error']:.4f}")
    print(f"PIDå‚æ•°ä¿æŒ: Kp={np.mean(kp_values):.2f}, Kd={np.mean(kd_values):.2f}")
    
    return results


def evaluate_meta_rl(robot_urdf, model_path, steps=10000):
    """è¯„ä¼°Meta-PID+RLï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰"""
    print("\n" + "="*80)
    print("è¯„ä¼° 2: Meta-PID + RLï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    model = PPO.load(model_path)
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = MetaRLCombinedEnv(robot_urdf=robot_urdf, gui=False)
    
    obs, _ = env.reset()
    
    # è®°å½•æ•°æ®
    errors = []
    kp_values = []
    kd_values = []
    rewards = []
    actions = []
    
    for step in range(steps):
        # RLåŠ¨ä½œ
        action, _states = model.predict(obs, deterministic=True)
        
        obs, reward, terminated, truncated, info = env.step(action)
        
        errors.append(info['tracking_error'])
        kp_values.append(info['current_kp'])
        kd_values.append(info['current_kd'])
        rewards.append(reward)
        actions.append(action)
        
        if step % 2000 == 0:
            print(f"Step {step:5d}: error={info['tracking_error']:.4f}, "
                  f"Kp={info['current_kp']:.2f}, Kd={info['current_kd']:.2f}, "
                  f"action={action}")
        
        if terminated or truncated:
            obs, _ = env.reset()
    
    env.close()
    
    results = {
        'errors': np.array(errors),
        'kp_values': np.array(kp_values),
        'kd_values': np.array(kd_values),
        'rewards': np.array(rewards),
        'actions': np.array(actions),
        'total_reward': np.sum(rewards),
        'mean_error': np.mean(errors),
        'median_error': np.median(errors),
        'max_error': np.max(errors)
    }
    
    print(f"\nMeta-PID+RL æ€»å¥–åŠ±: {results['total_reward']:.2f}")
    print(f"Meta-PID+RL å¹³å‡è¯¯å·®: {results['mean_error']:.4f}")
    print(f"Meta-PID+RL ä¸­ä½è¯¯å·®: {results['median_error']:.4f}")
    print(f"Meta-PID+RL æœ€å¤§è¯¯å·®: {results['max_error']:.4f}")
    print(f"PIDå‚æ•°èŒƒå›´: Kp=[{np.min(kp_values):.2f}, {np.max(kp_values):.2f}], "
          f"Kd=[{np.min(kd_values):.2f}, {np.max(kd_values):.2f}]")
    
    return results


def plot_comparison(pure_results, rl_results, save_path='meta_rl_comparison.png'):
    """ç»˜åˆ¶å¯¹æ¯”å›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # å®šä¹‰ç»Ÿä¸€é…è‰²
    color_pure = '#2E86AB'  # è“è‰² - Pure Meta-PID
    color_rl = '#F77F00'    # æ©™è‰² - Meta-PID + RL
    
    # 1. è·Ÿè¸ªè¯¯å·®å¯¹æ¯”
    ax = axes[0, 0]
    window = 100
    pure_smooth = np.convolve(pure_results['errors'], 
                               np.ones(window)/window, mode='valid')
    rl_smooth = np.convolve(rl_results['errors'], 
                             np.ones(window)/window, mode='valid')
    
    ax.plot(pure_smooth, color=color_pure, label='Pure Meta-PID', 
            alpha=0.8, linewidth=2)
    ax.plot(rl_smooth, color=color_rl, label='Meta-PID + RL', 
            alpha=0.8, linewidth=2)
    ax.set_xlabel('Time Step', fontweight='bold')
    ax.set_ylabel('Tracking Error (normalized)', fontweight='bold')
    ax.set_title('Tracking Error Comparison', fontweight='bold')
    ax.legend(framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # 2. å¥–åŠ±å¯¹æ¯”
    ax = axes[0, 1]
    pure_reward_smooth = np.convolve(pure_results['rewards'], 
                                      np.ones(window)/window, mode='valid')
    rl_reward_smooth = np.convolve(rl_results['rewards'], 
                                    np.ones(window)/window, mode='valid')
    
    ax.plot(pure_reward_smooth, color=color_pure, label='Pure Meta-PID', 
            alpha=0.8, linewidth=2)
    ax.plot(rl_reward_smooth, color=color_rl, label='Meta-PID + RL', 
            alpha=0.8, linewidth=2)
    ax.set_xlabel('Time Step', fontweight='bold')
    ax.set_ylabel('Reward', fontweight='bold')
    ax.set_title('Reward Comparison', fontweight='bold')
    ax.legend(framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # 3. KpåŠ¨æ€è°ƒæ•´
    ax = axes[1, 0]
    # çº¯Meta-PIDï¼šå›ºå®šå€¼ï¼Œç”¨è“è‰²è™šçº¿è¡¨ç¤º
    pure_kp_mean = np.mean(pure_results['kp_values'][:2000])
    ax.axhline(pure_kp_mean, color=color_pure, linestyle='--', linewidth=2.5, 
               label=f'Pure Meta-PID (fixed at {pure_kp_mean:.1f})', alpha=0.8, zorder=5)
    
    # Meta-PID+RLï¼šåŠ¨æ€è°ƒæ•´ï¼Œç”¨æ©™è‰²å®çº¿+å¡«å……åŒºåŸŸ
    rl_kp = rl_results['kp_values'][:2000]
    ax.plot(rl_kp, color=color_rl, linewidth=2, 
            label='Meta-PID + RL (adaptive)', alpha=0.9, zorder=3)
    # æ·»åŠ å¡«å……åŒºåŸŸæ˜¾ç¤ºRLçš„è°ƒæ•´èŒƒå›´
    ax.fill_between(range(len(rl_kp)), pure_kp_mean, rl_kp, 
                    color=color_rl, alpha=0.2, zorder=1)
    
    ax.set_xlabel('Time Step', fontweight='bold')
    ax.set_ylabel('Kp', fontweight='bold')
    ax.set_title('Kp Adjustment (First Episode)', fontweight='bold')
    ax.legend(loc='best', framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # 4. KdåŠ¨æ€è°ƒæ•´
    ax = axes[1, 1]
    # çº¯Meta-PIDï¼šå›ºå®šå€¼ï¼Œç”¨è“è‰²è™šçº¿è¡¨ç¤º
    pure_kd_mean = np.mean(pure_results['kd_values'][:2000])
    ax.axhline(pure_kd_mean, color=color_pure, linestyle='--', linewidth=2.5, 
               label=f'Pure Meta-PID (fixed at {pure_kd_mean:.1f})', alpha=0.8, zorder=5)
    
    # Meta-PID+RLï¼šåŠ¨æ€è°ƒæ•´ï¼Œç”¨æ©™è‰²å®çº¿+å¡«å……åŒºåŸŸ
    rl_kd = rl_results['kd_values'][:2000]
    ax.plot(rl_kd, color=color_rl, linewidth=2, 
            label='Meta-PID + RL (adaptive)', alpha=0.9, zorder=3)
    # æ·»åŠ å¡«å……åŒºåŸŸæ˜¾ç¤ºRLçš„è°ƒæ•´èŒƒå›´
    ax.fill_between(range(len(rl_kd)), pure_kd_mean, rl_kd, 
                    color=color_rl, alpha=0.2, zorder=1)
    
    ax.set_xlabel('Time Step', fontweight='bold')
    ax.set_ylabel('Kd', fontweight='bold')
    ax.set_title('Kd Adjustment (First Episode)', fontweight='bold')
    ax.legend(loc='best', framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\nğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")


def main():
    robot_urdf = 'franka_panda/panda.urdf'
    model_path = 'logs/meta_rl_panda/best_model/best_model'
    steps = 10000
    
    print("="*80)
    print("Meta-PID + RL ç»„åˆæ§åˆ¶å™¨è¯„ä¼°")
    print("="*80)
    print(f"æœºå™¨äºº: {robot_urdf}")
    print(f"è¯„ä¼°æ­¥æ•°: {steps}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    # è¯„ä¼°1: çº¯Meta-PID
    pure_results = evaluate_pure_meta_pid(robot_urdf, steps)
    
    # è¯„ä¼°2: Meta-PID + RL
    rl_results = evaluate_meta_rl(robot_urdf, model_path, steps)
    
    # æ€§èƒ½å¯¹æ¯”
    print("\n" + "="*80)
    print("æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("="*80)
    
    reward_improvement = (rl_results['total_reward'] - pure_results['total_reward']) / abs(pure_results['total_reward']) * 100
    error_improvement = (pure_results['mean_error'] - rl_results['mean_error']) / pure_results['mean_error'] * 100
    
    print(f"\nå¥–åŠ±æ”¹å–„: {rl_results['total_reward'] - pure_results['total_reward']:+.2f} ({reward_improvement:+.2f}%)")
    print(f"è¯¯å·®é™ä½: {pure_results['mean_error'] - rl_results['mean_error']:.4f} ({error_improvement:+.2f}%)")
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    plot_comparison(pure_results, rl_results)
    
    print("\n" + "="*80)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("="*80)


if __name__ == '__main__':
    main()

