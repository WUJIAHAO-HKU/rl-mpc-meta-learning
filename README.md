# RL-Enhanced Model Predictive Control with Meta-Learning

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶åŠ¨åŠ›å­¦æ¨¡å‹è¯¯å·®åœ¨çº¿è¡¥å¿æ–¹æ³•çš„å®˜æ–¹å®ç°ã€‚

## ğŸ“– å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```bibtex
@article{wu2025rl,
  title={Reinforcement Learning-Enhanced Model Predictive Control with Meta-Learning for Online Compensation of Dynamic Model Errors},
  author={Wu, Jiahao and others},
  journal={To be published},
  year={2025},
  note={Manuscript in preparation}
}
```

## ğŸŒŸ ä¸»è¦ç‰¹ç‚¹

- **å…ƒå­¦ä¹ ç½‘ç»œ**ï¼šå¿«é€Ÿè‡ªé€‚åº”PIDå‚æ•°é¢„æµ‹
- **å¼ºåŒ–å­¦ä¹ å¢å¼º**ï¼šåœ¨çº¿è¡¥å¿åŠ¨åŠ›å­¦æ¨¡å‹è¯¯å·®
- **å¤šæœºå™¨äººå¹³å°**ï¼šæ”¯æŒFranka Pandaï¼ˆ9-DOFä¸²è”ï¼‰å’ŒLaikagoï¼ˆ12-DOFå¹¶è”å››è¶³ï¼‰
- **é²æ£’æ€§éªŒè¯**ï¼šæŠ—å¤–éƒ¨æ‰°åŠ¨å’Œæ¨¡å‹ä¸ç¡®å®šæ€§
- **æ•°æ®å¢å¼º**ï¼šåŸºäºç‰©ç†çº¦æŸçš„è™šæ‹Ÿæ ·æœ¬ç”Ÿæˆ

## ğŸ’¬ Language Notes

- **Documentation**: Full English and Chinese documentation provided
- **Code Comments**: Core modules (`src/networks/`, `src/environments/base_env.py`) have complete English docstrings and comments
- **User-facing APIs**: All public functions and classes have English documentation
- **Some files**: May contain Chinese comments (legacy from development)
- **Contributions Welcome**: We welcome pull requests to improve internationalization

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- PyBullet
- NumPy, Matplotlib

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/WUJIAHAO-HKU/rl-mpc-meta-learning.git
cd rl-mpc-meta-learning

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### åŸºç¡€ä½¿ç”¨

#### 1. è®­ç»ƒå…ƒå­¦ä¹ ç½‘ç»œ

```bash
# ä½¿ç”¨æ•°æ®å¢å¼ºè®­ç»ƒ
python train_with_augmentation.py

# æˆ–ä½¿ç”¨åŸå§‹æ•°æ®è®­ç»ƒ
python train_meta_pid.py
```

#### 2. è®­ç»ƒRLç­–ç•¥

```bash
# Franka Pandaå¹³å°
python train_meta_rl_combined.py --robot franka --timesteps 1000000

# Laikagoå¹³å°
python train_meta_rl_combined.py --robot laikago --timesteps 1000000
```

#### 3. è¯„ä¼°æ€§èƒ½

```bash
# è¯„ä¼°Franka Panda
python evaluate_meta_rl.py --robot franka --model best_franka_model.zip

# è¯„ä¼°Laikago
python evaluate_laikago.py --model best_laikago_model.zip

# é²æ£’æ€§æµ‹è¯•
python evaluate_robustness.py --robot franka --disturbance_level 0.3
```

#### 4. ç”Ÿæˆå¯è§†åŒ–ç»“æœ

```bash
# ç”Ÿæˆæ‰€æœ‰è®ºæ–‡å›¾è¡¨
python generate_all_figures_unified.py

# å¯è§†åŒ–è®­ç»ƒæ›²çº¿
python visualize_training_curves.py --log training_log.txt
```

## ğŸ“Š å®éªŒç»“æœ

### Franka Panda (9-DOF)
| æŒ‡æ ‡ | Meta-PID | Meta-PID+RL | æ”¹è¿›ç‡ |
|------|----------|-------------|--------|
| MAE (Â°) | 7.51 | **6.26** | +16.6% |
| RMSE (Â°) | 29.32 | **25.45** | +13.2% |

### Laikago (12-DOF)
| æŒ‡æ ‡ | Meta-PID | Meta-PID+RL | æ”¹è¿›ç‡ |
|------|----------|-------------|--------|
| MAE (Â°) | 5.91 | **5.91** | 0.0% |
| RMSE (Â°) | 13.80 | **13.74** | +0.4% |

*æ³¨ï¼šLaikagoå¹³å°çš„0.0% MAEæ”¹è¿›ç‡åæ˜ äº†"ä¼˜åŒ–å¤©èŠ±æ¿æ•ˆåº”"â€”â€”å½“å…ƒå­¦ä¹ åŸºçº¿å·²è¾¾åˆ°è¿‘ä¹æœ€ä¼˜æ—¶ï¼ŒRLçš„è¾¹é™…æ”¶ç›Šå—é™ã€‚*

## ğŸ“ é¡¹ç›®ç»“æ„

```
rl-mpc-meta-learning/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ networks/          # ç¥ç»ç½‘ç»œæ¶æ„
â”‚   â”œâ”€â”€ environments/      # PyBulletä»¿çœŸç¯å¢ƒ
â”‚   â”œâ”€â”€ training/          # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluation/        # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ visualization/     # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ augmented_pid_data.json      # å¢å¼ºè®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ best_configs_paper.json      # æœ€ä½³é…ç½®
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶
â”œâ”€â”€ models/                # é¢„è®­ç»ƒæ¨¡å‹
â”œâ”€â”€ results/               # å®éªŒç»“æœ
â”œâ”€â”€ tests/                 # å•å…ƒæµ‹è¯•
â””â”€â”€ README.md
```

## ğŸ”¬ æ ¸å¿ƒç®—æ³•

### 1. å…ƒå­¦ä¹ ç½‘ç»œæ¶æ„

```python
class MetaPIDNetwork(nn.Module):
    """
    è¾“å…¥: æœºå™¨äººçŠ¶æ€ s_t = [q, q_dot, q_ref, q_ref_dot]
    è¾“å‡º: PIDå¢ç›Š [K_p, K_i, K_d] (æ¯ä¸ªå…³èŠ‚)
    """
    def __init__(self, state_dim, output_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        self.pid_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim),
            nn.Softplus()  # ç¡®ä¿PIDå¢ç›Šä¸ºæ­£
        )
```

### 2. RLç­–ç•¥ç½‘ç»œ

```python
class RLPolicy(nn.Module):
    """
    è¾“å…¥: å¢å¼ºçŠ¶æ€ [s_t, K_p, K_i, K_d, tracking_error]
    è¾“å‡º: è¡¥å¿åŠ›çŸ© Î´Ï„
    """
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Tanh()  # é™åˆ¶åŠ¨ä½œèŒƒå›´
        )
```

### 3. å¥–åŠ±å‡½æ•°è®¾è®¡

```python
def compute_reward(tracking_error, action, prev_error):
    """
    å¤šç›®æ ‡å¥–åŠ±å‡½æ•°
    """
    # è·Ÿè¸ªè¯¯å·®æƒ©ç½š
    r_tracking = -np.linalg.norm(tracking_error)
    
    # åŠ¨ä½œå¹³æ»‘æ€§æƒ©ç½š
    r_smoothness = -0.1 * np.linalg.norm(action)
    
    # è¯¯å·®å‡å°‘å¥–åŠ±
    r_improvement = 10.0 * (np.linalg.norm(prev_error) - 
                           np.linalg.norm(tracking_error))
    
    return r_tracking + r_smoothness + r_improvement
```

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æœºå™¨äººå¹³å°

```python
from src.environments.base_env import BaseRobotEnv

class CustomRobotEnv(BaseRobotEnv):
    def __init__(self):
        super().__init__(
            urdf_path="path/to/robot.urdf",
            n_joints=7,
            max_torque=100.0
        )
    
    def compute_dynamics(self, q, q_dot):
        # å®ç°æ‚¨çš„åŠ¨åŠ›å­¦æ¨¡å‹
        pass
```

### æ•°æ®å¢å¼ºé…ç½®

ç¼–è¾‘ `configs/augmentation_config.yaml`:

```yaml
augmentation:
  enabled: true
  samples_per_real: 10
  noise_levels:
    state: 0.01
    pid_gains: 0.05
  physics_constraints:
    enforce_stability: true
    check_controllability: true
```

## ğŸ“ˆ è®­ç»ƒæŠ€å·§

1. **å…ƒå­¦ä¹ ç½‘ç»œé¢„è®­ç»ƒ**ï¼šä½¿ç”¨å¤§é‡ç¦»çº¿æ•°æ®é¢„è®­ç»ƒä»¥è·å¾—è‰¯å¥½åˆå§‹åŒ–
2. **åˆ†é˜¶æ®µè®­ç»ƒ**ï¼šå…ˆè®­ç»ƒå…ƒå­¦ä¹ ç½‘ç»œï¼Œå†è®­ç»ƒRLç­–ç•¥
3. **è¯¾ç¨‹å­¦ä¹ **ï¼šä»ç®€å•è½¨è¿¹é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚è½¨è¿¹
4. **è¶…å‚æ•°è°ƒä¼˜**ï¼šä½¿ç”¨æˆ‘ä»¬æä¾›çš„æœ€ä½³é…ç½® `best_configs_paper.json`

## ğŸ§ª å¤ç°è®ºæ–‡ç»“æœ

```bash
# å®Œæ•´æµç¨‹ï¼ˆçº¦éœ€24å°æ—¶ï¼Œå•GPUï¼‰
bash scripts/reproduce_paper_results.sh

# æˆ–åˆ†æ­¥æ‰§è¡Œï¼š
# Step 1: è®­ç»ƒå…ƒå­¦ä¹ ç½‘ç»œ
python train_with_augmentation.py

# Step 2: è®­ç»ƒFranka RLç­–ç•¥
python train_meta_rl_combined.py --robot franka --timesteps 1000000

# Step 3: è®­ç»ƒLaikago RLç­–ç•¥
python train_meta_rl_combined.py --robot laikago --timesteps 1000000

# Step 4: è¯„ä¼°å¹¶ç”Ÿæˆç»“æœ
python evaluate_meta_rl.py --robot franka
python evaluate_laikago.py
python generate_all_figures_unified.py
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯Pull Request

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ“§ è”ç³»æ–¹å¼

- ä½œè€…ï¼š[WU JIAHAO]
- é‚®ç®±ï¼š[u3661739@connect.hku.hk]
- é¡¹ç›®ä¸»é¡µï¼š[https://github.com/WUJIAHAO-HKU/rl-mpc-meta-learning](https://github.com/WUJIAHAO-HKU/rl-mpc-meta-learning)

## ğŸ™ è‡´è°¢

- PyBulletå›¢é˜Ÿæä¾›çš„ä¼˜ç§€ç‰©ç†ä»¿çœŸå¼•æ“
- Stable-Baselines3æä¾›çš„RLç®—æ³•å®ç°
- åŒ¿åå®¡ç¨¿äººçš„å®è´µæ„è§

## ğŸ“š ç›¸å…³å·¥ä½œ

å¦‚æœæ‚¨å¯¹æœ¬ç ”ç©¶æ„Ÿå…´è¶£ï¼Œå¯èƒ½ä¹Ÿä¼šå¯¹ä»¥ä¸‹å·¥ä½œæ„Ÿå…´è¶£ï¼š

- [Meta-Learning for Control](https://arxiv.org/abs/xxxx.xxxxx)
- [Model Predictive Control with Neural Networks](https://arxiv.org/abs/xxxx.xxxxx)
- [Reinforcement Learning for Robotics](https://arxiv.org/abs/xxxx.xxxxx)

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªStarï¼â­**

